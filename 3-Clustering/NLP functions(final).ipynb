{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP functions(final).ipynb","provenance":[{"file_id":"1FGbWYYpDG09vDVs6sfJFJ7nb5vua5Bu3","timestamp":1587364381804}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"VD18J8MOW1UJ","colab_type":"code","outputId":"f3054998-baff-4301-a157-b13e074c5106","executionInfo":{"status":"ok","timestamp":1587050453600,"user_tz":-120,"elapsed":948,"user":{"displayName":"Induraj Pudhupattu Ramamurthy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgPlhL-19UZ0BvQi7JTFYs76zIek5JodoKlI1EqOA=s64","userId":"17678391171202282033"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Libraries\n","import numpy as np \n","import pandas as pd\n","import re\n","import nltk\n","\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection  import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import metrics\n","from sklearn.utils import resample\n","\n","from sklearn.preprocessing import label_binarize\n","\n","from sklearn.cluster import KMeans\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE \n","from sklearn.decomposition import TruncatedSVD\n","\n","import gensim\n","\n","import matplotlib.pyplot as plt\n","from nltk.tokenize import sent_tokenize"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ssVwnaJaWk_t","colab_type":"text"},"source":["**To Clean, Normalize (text preprocessing)**"]},{"cell_type":"code","metadata":{"id":"hqvHcxk0WYle","colab_type":"code","colab":{}},"source":["stop_words = nltk.corpus.stopwords.words('portuguese')\n","tokenizer = RegexpTokenizer(r'\\w+')\n","stemmer = SnowballStemmer('portuguese')\n","#word_token_final_list_st =[]\n","\n","def cleantext(text):\n","  lean1Text = re.sub('\\(.*?\\)','',text)\n","  month = \"(janeiro|fevereiro|marco|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\"\n","  #lean1Text = re.sub('\\d{1, }\\sde\\s'+month+'\\sde\\s\\d{2,}','',lean1Text)\n","  lean1Text = re.sub('(\\s\\d{1,}\\sde\\s'+month+'\\sde\\s\\d{4})', ' ', lean1Text)\n","  lean1Text = re.sub('(\\d{1,}\\sde\\s'+month+'\\sde\\s\\d{4})', ' ', lean1Text)\n","  lean1Text = re.sub('(\\sde\\s'+month+'\\sde\\s\\d{4})', ' ', lean1Text)\n","  lean1Text = re.sub('(\\s'+month+'\\sde\\s\\d{4})', ' ', lean1Text)\n","  lean1Text = re.sub('mil',' ',lean1Text)\n","  lean1Text = re.sub('r\\$[\\s+]?[\\d+]?[\\.]?[\\,]?[\\d{1,}]?[\\.]?[\\d{1,}]?[\\.]?[\\,]?[\\d{1,}]?',' ',lean1Text)\n","  lean1Text = re.sub('\\(.*?\\)',' ',lean1Text)\n","  lean1Text = re.sub('\\sde[\\s{1,}]?de\\s', ' ',lean1Text)\n","  lean1Text = re.sub('\\sno[\\s]?[\\d]{1,}\\/[\\d]{1,}',' ',lean1Text)\n","  lean1Text = re.sub('\\s\\d\\.\\s',' ',lean1Text)\n","  lean1Text = re.sub('\\s\\w\\.\\s',' ',lean1Text)\n","  lean1Text = re.sub('\\sno[\\s]?[\\d]{1,}[^\\w]?[\\d]{1,}[\\S\\W\\D]?[\\d]{1,}',' ',lean1Text)\n","  lean1Text = re.sub('\\sn[\\.]?[\\s]?[\\d]{1,}[^\\w]?[\\d]{1,}[\\S\\W\\D]?[\\d]{1,}',' ',lean1Text)\n","  lean1Text = re.sub('de[\\s][\\.][\\s]lei[\\s][\\W][\\D]de[\\s][\\W][\\D]art[\\.]' , '\\sde\\s\\lei\\sart\\s' ,lean1Text)\n","  lean1Text = re.sub('[\\s][\\.][\\s]lei[\\s][\\W][\\D]de[\\s][\\W][\\D]art[\\.]','\\slei\\s\\de\\sart\\s',lean1Text)\n","  lean1Text = re.sub('(\\s[a-zA-Z]\\)\\s)|(\\s[a-zA-Z]\\-\\s)',' ',lean1Text)\n","  lean1Text = re.sub('(http[s]?...\\w*.?\\w*\\.?\\w*.*?\\s)|[^\\w\\s{1}\\.\\?\\!]|[\\d]',' ',lean1Text)\n","  lean1Text = re.sub('(\\si{1,}[v]?[\\W])|(\\svi{1,}[\\W])|(\\six[\\W])|(xi{1,}[\\W])|(x{1,}[\\W])',' ',lean1Text)\n","  lean1Text = re.sub('\\s\\v\\s',' ',lean1Text)\n","  lean1Text = re.sub('\\sde\\s[\\W]',' ',lean1Text)\n","  lean1Text = re.sub( '\\s[\\W]\\d{1,}[\\W]\\d{1,}[\\W]',' ',lean1Text)\n","  lean1Text = re.sub( '\\sarea\\s{1,}[\\W]',' ',lean1Text)\n","  lean1Text = re.sub('\\scplc\\sdepcon\\su\\spgf\\sagu\\s',' ',lean1Text)\n","  lean1Text = re.sub('\\slei\\sn\\.o\\s',' ',lean1Text)\n","  lean1Text=re.sub('(\\sm2\\s)|(\\szm\\s)',' ',lean1Text)\n","  lean1Text= re.sub('\\.\\.+' , '.',lean1Text)\n","  lean1Text= re.sub('\\.+', '.', lean1Text)\n","  lean1Text= re.sub('\\s\\.+','.',lean1Text)\n","  lean1Text=re.sub('\\s\\.','.',lean1Text)\n","  lean1Text=re.sub('\\s\\.\\s{1,}','.',lean1Text)\n","  lean1Text=re.sub('(\\.em\\s)|(\\sem\\.)','\\sem\\s',lean1Text)\n","  lean1Text=re.sub('\\s{2,}',' ',lean1Text)\n","  lean1Text=re.sub('\\.{2,}',' ',lean1Text)\n","  lean1Text=re.sub('\\s\\.\\s{1,}','.',lean1Text)\n","  lean1Text=re.sub('\\.\\s\\.','.',lean1Text)\n","  lean1Text=re.sub('(\\skm\\sa\\s)|(\\se\\skm\\s)|(\\sao\\skm\\s)|(\\ssendo\\skm\\s)|(\\sao\\skm)|(\\se\\skm)|(\\sao\\skm)',' ',lean1Text)\n","\n","  return lean1Text.strip()\n","\n","def tokenize_word(cleaned_list):\n","  tokens_class=nlp(cleaned_list)                                    # https://spacy.io/models/pt          # token.text , token.pos_   , token.dep_      #https://lars76.github.io/nlp/lemmatize-portuguese/   (why lemmatization is bad in portugesse case)\n","  token_list=[token.text.strip() for token in tokens_class if str(token)!=' ']\n","  return token_list\n","\n","def normalize(corpus,stem=True,no_stem=True):\n","  word_token_final_list_stemmed =[]\n","  word_token_final_list_non_stemmed=[]\n","  for sentence in corpus:\n","    sentence= sentence[0:1000000]\n","    sentence= sentence.lower()\n","    cleaned_list=cleantext(sentence)\n","    token_list_main=tokenize_word(cleaned_list)\n","    without_stop_word_list = [x for x in token_list_main if x not in stop_words]\n","    new_pattern= r'(\\s\\w\\s)|(^\\w\\s)'\n","    \n","    if stem==True and no_stem==False:\n","      stemmed_list = [stemmer.stem(x) for x in without_stop_word_list]\n","      stemmed_text = ' '.join(stemmed_list)\n","      \n","      stemmed_text_final= re.sub(new_pattern,'',stemmed_text)\n","      word_token_final_list_stemmed.append(stemmed_text_final)\n","      word_token_final_list_non_stemmed.append('NaN')\n","    \n","    elif stem==False and no_stem==True:\n","      without_stop_word_text = ' '.join(element for element in without_stop_word_list)\n","      non_stemmed_text_final= re.sub(new_pattern,'',without_stop_word_text)\n","      word_token_final_list_non_stemmed.append(non_stemmed_text_final)\n","      word_token_final_list_stemmed.append('NaN')\n","     \n","    else:\n","      stemmed_list = [stemmer.stem(x) for x in without_stop_word_list]\n","      stemmed_text = ' '.join(stemmed_list)\n","      new_pattern= r'(\\s\\w\\s)|(^\\w\\s)'\n","      stemmed_text_final= re.sub(new_pattern,'',stemmed_text)\n","      word_token_final_list_stemmed.append(stemmed_text_final)\n","\n","      without_stop_word_text = ' '.join(element for element in without_stop_word_list)\n","      non_stemmed_text_final= re.sub(new_pattern,'',without_stop_word_text)\n","      word_token_final_list_non_stemmed.append(non_stemmed_text_final)\n","\n","  return word_token_final_list_stemmed, word_token_final_list_non_stemmed"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h21YWSZvJRgg","colab_type":"text"},"source":["Sentence tokenizer"]},{"cell_type":"code","metadata":{"id":"d42I4Dzj_SuP","colab_type":"code","colab":{}},"source":["from nltk.tokenize import sent_tokenize\n","stop_words = nltk.corpus.stopwords.words('portuguese')\n","#tokenizer = RegexpTokenizer(r'\\w+')\n","#stemmer = SnowballStemmer('portuguese')\n","#word_token_final_list_st =[]\n","\n","def cleantext1(text):\n","  lean1Text = re.sub('\\(.*?\\)','',text)\n","  month = \"(janeiro|fevereiro|marco|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\"\n","  #lean1Text = re.sub('\\d{1, }\\sde\\s'+month+'\\sde\\s\\d{2,}','',lean1Text)\n","  lean1Text = re.sub('(\\s\\d{1,}\\sde\\s'+month+'\\sde\\s\\d{4})', ' ', lean1Text)\n","  lean1Text = re.sub('(\\d{1,}\\sde\\s'+month+'\\sde\\s\\d{4})', ' ', lean1Text)\n","  lean1Text = re.sub('(\\sde\\s'+month+'\\sde\\s\\d{4})', ' ', lean1Text)\n","  lean1Text = re.sub('(\\s'+month+'\\sde\\s\\d{4})', ' ', lean1Text)\n","  lean1Text = re.sub('mil',' ',lean1Text)\n","  lean1Text = re.sub('r\\$[\\s+]?[\\d+]?[\\.]?[\\,]?[\\d{1,}]?[\\.]?[\\d{1,}]?[\\.]?[\\,]?[\\d{1,}]?',' ',lean1Text)\n","  lean1Text = re.sub('\\(.*?\\)',' ',lean1Text)\n","  lean1Text = re.sub('\\sde[\\s{1,}]?de\\s', ' ',lean1Text)\n","  lean1Text = re.sub('\\sno[\\s]?[\\d]{1,}\\/[\\d]{1,}',' ',lean1Text)\n","  lean1Text = re.sub('\\s\\d\\.\\s',' ',lean1Text)\n","  lean1Text = re.sub('\\s\\w\\.\\s',' ',lean1Text)\n","  lean1Text = re.sub('\\sno[\\s]?[\\d]{1,}[^\\w]?[\\d]{1,}[\\S\\W\\D]?[\\d]{1,}',' ',lean1Text)\n","  lean1Text = re.sub('\\sn[\\.]?[\\s]?[\\d]{1,}[^\\w]?[\\d]{1,}[\\S\\W\\D]?[\\d]{1,}',' ',lean1Text)\n","  lean1Text = re.sub('de[\\s][\\.][\\s]lei[\\s][\\W][\\D]de[\\s][\\W][\\D]art[\\.]' , '\\sde\\s\\lei\\sart\\s' ,lean1Text)\n","  lean1Text = re.sub('[\\s][\\.][\\s]lei[\\s][\\W][\\D]de[\\s][\\W][\\D]art[\\.]','\\slei\\s\\de\\sart\\s',lean1Text)\n","  lean1Text = re.sub('(\\s[a-zA-Z]\\)\\s)|(\\s[a-zA-Z]\\-\\s)',' ',lean1Text)\n","  lean1Text = re.sub('(http[s]?...\\w*.?\\w*\\.?\\w*.*?\\s)|[^\\w\\s{1}\\.\\?\\!]|[\\d]',' ',lean1Text)\n","  lean1Text = re.sub('(\\si{1,}[v]?[\\W])|(\\svi{1,}[\\W])|(\\six[\\W])|(xi{1,}[\\W])|(x{1,}[\\W])',' ',lean1Text)\n","  lean1Text = re.sub('\\s\\v\\s',' ',lean1Text)\n","  lean1Text = re.sub('\\sde\\s[\\W]',' ',lean1Text)\n","  lean1Text = re.sub( '\\s[\\W]\\d{1,}[\\W]\\d{1,}[\\W]',' ',lean1Text)\n","  lean1Text = re.sub( '\\sarea\\s{1,}[\\W]',' ',lean1Text)\n","  lean1Text = re.sub('\\scplc\\sdepcon\\su\\spgf\\sagu\\s',' ',lean1Text)\n","  lean1Text = re.sub('\\slei\\sn\\.o\\s',' ',lean1Text)\n","  lean1Text=re.sub('(\\sm2\\s)|(\\szm\\s)',' ',lean1Text)\n","  lean1Text= re.sub('\\.\\.+' , '.',lean1Text)\n","  lean1Text= re.sub('\\.+', '.', lean1Text)\n","  lean1Text= re.sub('\\s\\.+','.',lean1Text)\n","  lean1Text=re.sub('\\s\\.','.',lean1Text)\n","  lean1Text=re.sub('\\s\\.\\s{1,}','.',lean1Text)\n","  lean1Text=re.sub('(\\.em\\s)|(\\sem\\.)','\\sem\\s',lean1Text)\n","  lean1Text=re.sub('\\s{2,}',' ',lean1Text)\n","  lean1Text=re.sub('\\.{2,}',' ',lean1Text)\n","  lean1Text=re.sub('\\s\\.\\s{1,}','.',lean1Text)\n","  lean1Text=re.sub('\\.\\s\\.','.',lean1Text)\n","  lean1Text=re.sub('(\\skm\\sa\\s)|(\\se\\skm\\s)|(\\sao\\skm\\s)|(\\ssendo\\skm\\s)|(\\sao\\skm)|(\\se\\skm)|(\\sao\\skm)',' ',lean1Text)\n","\n","  return lean1Text.strip()\n","\n","def tokenize_word1(cleaned_list):\n","  tokens_class=nlp(cleaned_list)                                    # https://spacy.io/models/pt          # token.text , token.pos_   , token.dep_      #https://lars76.github.io/nlp/lemmatize-portuguese/   (why lemmatization is bad in portugesse case)\n","  token_list=[token.text.strip() for token in tokens_class if str(token)!=' ']\n","  return token_list\n","\n","def normalize1(corpus):\n","  c=0;\n","  word_token_final_list_sent_tokenized=[]\n","  for sentence in corpus:\n","    sentence= sentence[0:1000000]\n","    sentence= sentence.lower()\n","    cleaned_list=cleantext1(sentence)\n","    sentence_tokenized_list = sent_tokenize(cleaned_list)\n","    with_stop_final_text = ' '.join(sentence_tokenized_list)\n","    word_token_final_list_sent_tokenized.append(with_stop_final_text)\n","  return word_token_final_list_sent_tokenized"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MPXFgmDW_aAK","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"XepfAtPk_XYz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2lXteLP14OeH","colab_type":"code","colab":{}},"source":["def pca_reduction(feature_matrix,convert_to_array='yes'):\n","  pca = PCA(n_components=2, random_state=0)\n","  if convert_to_array=='yes':  \n","    feature_matrix = pca.fit_transform(feature_matrix.toarray())\n","  else:\n","    feature_matrix = pca.fit_transform(feature_matrix)\n","  return feature_matrix\n","\n","def tsne_reduction(feature_matrix, convert_to_array='yes'):\n","  tsne = TSNE(n_components=2, random_state=0)\n","  np.set_printoptions(suppress=True)\n","  if convert_to_array=='yes':\n","    feature_matrix = tsne.fit_transform(feature_matrix.toarray())\n","  else:\n","    feature_matrix =  tsne.fit_transform(feature_matrix)\n","  return feature_matrix\n","\n","def svd(feature_matrix, convert_to_array='yes'):\n","  svd = TruncatedSVD(n_components=2, random_state=0)\n","  np.set_printoptions(suppress=True)\n","  if convert_to_array=='yes':\n","    feature_matrix = svd.fit_transform(feature_matrix.toarray())\n","  else:\n","    feature_matrix =  svd.fit_transform(feature_matrix)\n","  return feature_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GpA9dBwseFjF","colab_type":"text"},"source":["**TFIDF**"]},{"cell_type":"code","metadata":{"id":"ME5bXhlleEUO","colab_type":"code","colab":{}},"source":["def feature_engineering(feature_eng_method, corpus, min_df, max_df, n1, n2, dimension_reduction=True, reduction_method='pca'):\n","  if feature_eng_method=='tf_idf':\n","    vectorizer = TfidfVectorizer(min_df=min_df,max_df=max_df,ngram_range=(n1,n2))\n","    feature_matrix = vectorizer.fit_transform(corpus).astype(float)\n","  \n","  if dimension_reduction==True:\n","    if reduction_method =='pca':\n","      feature_matrix = pca_reduction(feature_matrix)\n","      return vectorizer,feature_matrix\n","  \n","    if reduction_method =='tsne':\n","      feature_matrix= tsne_reduction(feature_matrix)\n","      return vectorizer, feature_matrix\n","\n","    if reduction_method == 'svd':\n","      feature_matrix= svd(feature_matrix)\n","      return vectorizer, feature_matrix\n","  elif dimension_reduction==False:\n","    return vectorizer, feature_matrix      "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B5maYchZE6Y1","colab_type":"text"},"source":["**W2V**"]},{"cell_type":"markdown","metadata":{"id":"dtTwvSmaE-v8","colab_type":"text"},"source":["Avgerage w2v and tfidf w2v"]},{"cell_type":"code","metadata":{"id":"TEOYmiZjAwd4","colab_type":"code","colab":{}},"source":["def average_word_vectors(words, model, vocabulary, num_features):\n","  \n","    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n","    nwords = 0.\n","    \n","    for word in words:\n","        if word in vocabulary: \n","            nwords = nwords + 1.\n","            feature_vector = np.add(feature_vector, model[word])\n","    \n","    if nwords:\n","        feature_vector = np.divide(feature_vector, nwords)\n","        \n","    return feature_vector\n","    \n","def averaged_word_vectorizer(corpus, model, num_features):\n","    vocabulary = set(model.wv.index2word)\n","    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n","                    for tokenized_sentence in corpus]\n","    return np.array(features)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ucjKkAaSKsqx","colab_type":"text"},"source":["Unsupervised "]},{"cell_type":"code","metadata":{"id":"8rCK1rV_bpbp","colab_type":"code","colab":{}},"source":["from sklearn.cluster import KMeans\n","from sklearn.cluster import AffinityPropagation\n","from nltk.cluster import KMeansClusterer , euclidean_distance\n","from sklearn.cluster import DBSCAN\n","from sklearn.cluster import Birch\n","from sklearn.cluster import MiniBatchKMeans\n","\n","def ml_unsupervised(method,feature_matrix,numb_cluster=3):\n","  if method=='kmeans_scikit':\n","    km = KMeans(n_clusters=numb_cluster,max_iter=100,random_state=12)\n","    km.fit(feature_matrix)\n","    clusters = km.labels_\n","    return km, clusters\n","  if method=='kmeans_nltk':\n","    #km = KMeansClusterer(numb_cluster, distance=nltk.cluster.util.cosine_distance, repeats=25,avoid_empty_clusters=True)\n","    km = KMeansClusterer(numb_cluster, distance=euclidean_distance, repeats=25,avoid_empty_clusters=True)\n","    clusters = km.cluster(feature_matrix, assign_clusters=True)\n","    return km, clusters\n","  # if method=='affinity_propogation':\n","  #   ap= AffinityPropagation(n_clusters=numb_cluster)\n","  #   ap.fit(feature_matrix)\n","  #   clusters=ap.labels_\n","  #   return ap, clusters\n","  # if method=='dbscan':\n","  #   db=DBSCAN(eps=0.5, metric='euclidean', min_samples=10)\n","  #   db.fit(feature_matrix)\n","  #   clusters= db.labels_\n","  #   return db,clusters\n","  if method =='minibatchkmeans':\n","    mb=MiniBatchKMeans(n_clusters=numb_cluster,random_state=12)\n","    mb.fit(feature_matrix)\n","    clusters=mb.labels_\n","    return mb, clusters\n","\n","    \n","\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bc9GG8RIKplm","colab_type":"text"},"source":["Metrices"]},{"cell_type":"code","metadata":{"id":"Z_64mcpzKoae","colab_type":"code","colab":{}},"source":["def get_metrics(true_labels, predicted_labels):\n","  acc = np.round(metrics.accuracy_score(true_labels,predicted_labels),4)\n","  pre = np.round(metrics.precision_score(true_labels,predicted_labels,average='binary'),4)\n","  rec = np.round(metrics.recall_score(true_labels,predicted_labels,average='binary'),4)\n","  f1s = np.round(metrics.f1_score(true_labels,predicted_labels,average='binary'),4)\n","\n","  print ('Accuracy:', acc, ' Precision:', pre, ' Recall:', rec, ' F1 Score:', f1s )\n","  return acc, pre, rec, f1s\n","\n","def metrics_dataframe(cols):\n","  Type =[]\n","  Acc =[]\n","  Precision=[]\n","  Recall =[]\n","  f1 =[]\n","\n","  def get_metrics(true_labels, predicted_labels, ele):\n","    Type.append(ele)\n","    Acc.append(np.round(metrics.accuracy_score(true_labels,predicted_labels),2))\n","    Precision.append(np.round(metrics.precision_score(true_labels,predicted_labels,average='binary'),2))\n","    Recall.append(np.round(metrics.recall_score(true_labels,predicted_labels,average='binary'),2))\n","    f1.append(np.round(metrics.f1_score(true_labels,predicted_labels,average='binary'),2))\n","\n","  for ele in cols: \n","    get_metrics(binarized_risk_label, df[ele] , ele )\n","\n","  return Type,Acc,Precision,Recall,f1\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NPyfV6obURE5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mgsEX9dUw-8G","colab_type":"code","colab":{}},"source":["def custom_plot(feature_matrix, cluster_name):\n","  n = feature_matrix.shape[0]\n","  fig = plt.figure()\n","  fig.set_size_inches(5,5)\n","  ax1=fig.add_subplot(1,1,1)\n","  for i in range(0, len(feature_matrix)):\n","    if (df.iloc[i]['LABEL']=='Sem impacto negativo') or (df.iloc[i]['LABEL']=='Médio') or (df.iloc[i]['LABEL']=='Baixo') or (df.iloc[i]['LABEL']=='Falha Média') or (df.iloc[i]['LABEL']=='Falha Formal') and (df.iloc[i][cluster_name]==0):\n","      c1,=ax1.plot(feature_matrix[i,0],feature_matrix[i,1],c='b',marker='+')\n","    elif (df.iloc[i]['LABEL'] =='Alto') or (df.iloc[i]['LABEL']=='Muito Alto') and (df.iloc[i][cluster_name]==1):\n","      c2,=ax1.plot(feature_matrix[i,0],feature_matrix[i,1],c='r',marker='o')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_w3iDqyAu0R","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pZWZK9gjYmFx","colab_type":"code","colab":{}},"source":["def overSamplingRISKs(lcorpus, llabels):\n","  #OVERSAMPLING HIGH RISKs\n","  # concatenate our training data back together\n","  X = pd.DataFrame(list(zip(lcorpus, llabels)), \n","                columns =['ConstText', 'RISK']) \n","\n","  # separate minority and majority classes\n","  not_HIGH = X[X.RISK!=\"HIGH\"]\n","  HIGH = X[X.RISK==\"HIGH\"]\n","\n","  # OVERsample minority\n","  high_upsampled = resample(HIGH,\n","                            replace=True, # sample with replacement\n","                            n_samples=len(not_HIGH), # match number in majority class\n","                            random_state=27) # reproducible results\n","\n","  # combine majority and upsampled minority\n","  upsampled = pd.concat([not_HIGH, high_upsampled])\n","\n","  #Recreate TRAIN_Corpus and TRAIN_LABELS\n","  lcorpus1=upsampled.ConstText.to_list()\n","  llabels1=upsampled.RISK.to_list()\n","\n","  # check new class counts\n","  print( upsampled.RISK.value_counts() )\n","\n","  return lcorpus1, llabels1"],"execution_count":0,"outputs":[]}]}