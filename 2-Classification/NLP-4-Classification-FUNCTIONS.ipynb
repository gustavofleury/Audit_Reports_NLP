{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP-4-Classification-FUNCTIONS.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3fMmrMwXgRGx","colab_type":"text"},"source":["#Classification - Functions\n","\n","Authors: Gustavo FLEURY && Induraj RAMAMURTHY\n","\n","Project: https://github.com/gustavofleury/Audit_Reports_NLP"]},{"cell_type":"code","metadata":{"id":"YX1o5W-_feIX","colab_type":"code","outputId":"23a2d540-8b2d-4cf9-8cb4-de492ca7d738","executionInfo":{"status":"ok","timestamp":1581696899589,"user_tz":-60,"elapsed":3359,"user":{"displayName":"Gustavo Fleury","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDuWY-d1ndXTKSmrNRgPn5QMGqH9zM_HInyvxYPUzA=s64","userId":"12285669053920214143"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#Libraries\n","import numpy as np \n","import pandas as pd\n","import re\n","import nltk\n","\n","from nltk.stem.snowball import SnowballStemmer\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection  import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn import metrics\n","from sklearn.utils import resample\n","\n","from sklearn.decomposition import NMF\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import MaxAbsScaler \n","\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE \n","\n","from sklearn.preprocessing import label_binarize\n","\n","import gensim\n","\n","# try:\n","#   # Use the %tensorflow_version magic if in colab.\n","#   %tensorflow_version 2.x\n","# except Exception:\n","#   pass\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import matplotlib.style as style\n","\n","import pickle\n","\n","# !git clone https://github.com/facebookresearch/fastText.git\n","# %cd fastText\n","# !pip install .\n","# import fasttext\n","# import fasttext.util"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yt4BPD-igi_X","colab_type":"text"},"source":["#Normalize"]},{"cell_type":"code","metadata":{"id":"3gjPy9ypgeBR","colab_type":"code","colab":{}},"source":["#NORMALIZE_TOKEN\n","stop_words = nltk.corpus.stopwords.words('portuguese')\n","tokenizer = RegexpTokenizer(r'\\w+')\n","stemmer = SnowballStemmer('portuguese')\n","\n","def removeUnWantedWords(text):\n","\n","  # Remove numbers/dates/pages \n","  cleanText = re.sub('\\d', \" \", text) #Remove all Numbers\n","  # cleanText = re.sub('\\s\\d+|\\d+\\s', \" \", text)\n","\n","  # Remove months\n","  month = \"(janeiro|fevereiro|marco|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)\"\n","  cleanText = re.sub('\\s'+month+'\\s', \" \", cleanText)\n","  return cleanText.strip()\n","\n","def normalize_token_corpus(corpus, \n","                           stem=False,\n","                           stop_words_IF=True,\n","                           tokenize=True,\n","                           nbWords=0):\n","    \n","    normalized_token_corpus = []    \n","    for text in corpus:\n","        \n","        # Remove Unwanted Numbers/Dates/NumberPages\n","        text=removeUnWantedWords(text)\n","        \n","        # Tokenize the input string\n","        tokens = tokenizer.tokenize(text.lower())\n","\n","        # Take first 2xnbWords ot tokens (try to improve performance)\n","        if nbWords != 0:\n","          nbWords2 = 2*nbWords\n","          tokens = tokens[:nbWords2]   \n","\n","        # Remove the stop words \n","        if stop_words_IF:    \n","          tokens = [x for x in tokens if not x in stop_words]\n"," \n","        # Perform stemming on the tokenized words \n","        if stem:\n","          tokens = [stemmer.stem(x) for x in tokens]\n","\n","        # Take first nbWords ot tokens\n","        if nbWords != 0:\n","          tokens = tokens[:nbWords]   \n","\n","        if tokenize:\n","            normalized_token_corpus.append(tokens)\n","        else:\n","            tokens=\" \".join(tokens)\n","            normalized_token_corpus.append(tokens)\n","    \n","    return normalized_token_corpus"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PDn2BD_Xgr_R","colab_type":"code","colab":{}},"source":["#TF_IDF\n","def tf_idf(corpus,\n","           ngram_range=(1, 1),\n","           min_df=0.0,\n","           max_df=1.0):\n","  \n","  vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range)\n","  feature_matrix = vectorizer.fit_transform(corpus).astype(float)\n","\n","  return vectorizer, feature_matrix\n","\n","def display_features(features, feature_names):\n","    df = pd.DataFrame(data=features,\n","                      columns=feature_names)\n","    print(df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LFTGooBb0Qm","colab_type":"code","colab":{}},"source":["def feature_engineering(feature_eng_method, corpus, min_df, max_df, n1, n2, dimension_reduction=True, reduction_method='pca'):\n","  if feature_eng_method=='tf_idf':\n","    vectorizer = TfidfVectorizer(min_df=min_df,max_df=max_df,ngram_range=(n1,n2))\n","    feature_matrix = vectorizer.fit_transform(corpus).astype(float)\n","  \n","  if dimension_reduction==True:\n","    if reduction_method =='pca':\n","      feature_matrix = pca_reduction(feature_matrix)\n","      return vectorizer,feature_matrix\n","  \n","    if reduction_method =='tsne':\n","      feature_matrix= tsne_reduction(feature_matrix)\n","      return vectorizer, feature_matrix\n","\n","  elif dimension_reduction==False:\n","    return vectorizer, feature_matrix    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3i7S8VEPcIJu","colab_type":"code","colab":{}},"source":["def pca_reduction(feature_matrix,convert_to_array='yes'):\n","  pca = PCA(n_components=2, random_state=0)\n","  if convert_to_array=='yes':  \n","    feature_matrix = pca.fit_transform(feature_matrix.toarray())\n","  else:\n","    feature_matrix = pca.fit_transform(feature_matrix)\n","  return feature_matrix\n","\n","def tsne_reduction(feature_matrix, convert_to_array='yes'):\n","  tsne = TSNE(n_components=2, random_state=0)\n","  np.set_printoptions(suppress=True)\n","  if convert_to_array=='yes':\n","    feature_matrix = tsne.fit_transform(feature_matrix.toarray())\n","  else:\n","    feature_matrix =  tsne.fit_transform(feature_matrix)\n","  return feature_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fnZlqEY2gufe","colab_type":"code","colab":{}},"source":["# Word2Vector\n","# Create the model with:\n","def w2v_createModel(corpus_token):\n","  model = gensim.models.Word2Vec(corpus_token, min_count=2, window=30, size=500, sample=1e-3, iter=100)\n","  return model\n","\n","def w2v_applyModel(model, corpus_token):  # historical - keep the compability.\n","  return applyModel(model, corpus_token)\n","\n","def applyModel(model, corpus_token):  \n","  X=[]\n","  for sentence in corpus_token:\n","      X.append(sent_vectorizer(sentence, model)) \n","  return X\n","\n","def sent_vectorizer(sent, model):\n","    sent_vec =[]\n","    numw = 0\n","    for w in sent:\n","        try:\n","            if numw == 0:\n","                sent_vec = model[w]\n","            else:\n","                sent_vec = np.add(sent_vec, model[w])\n","            numw+=1\n","        except:\n","            pass\n","     \n","    return np.asarray(sent_vec) / numw\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a30W8pVVckio","colab_type":"code","colab":{}},"source":["def average_word_vectors(words, model, vocabulary, num_features):\n","  \n","    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n","    nwords = 0.\n","    \n","    for word in words:\n","        if word in vocabulary: \n","            nwords = nwords + 1.\n","            feature_vector = np.add(feature_vector, model[word])\n","    \n","    if nwords:\n","        feature_vector = np.divide(feature_vector, nwords)\n","        \n","    return feature_vector\n","    \n","def averaged_word_vectorizer(corpus, model, num_features):\n","    vocabulary = set(model.wv.index2word)\n","    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n","                    for tokenized_sentence in corpus]\n","    return np.array(features)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9uDE1O8Ig9uW","colab_type":"code","colab":{}},"source":["def clean_features_W2V(lfeature, lLabels):  #Delete Vectors with small size.\n","  nFeatures = lfeature\n","  nLabels = lLabels\n","  maxLen =  max( [len(x) for x in lfeature] )\n","  for i in range( len(lfeature)-1, -1, -1) : #Descendre loop for pop()\n","    if len(lfeature[i]) != maxLen:\n","      # print( i, \" \", len(lfeature[i]), \" \", lLabels[i] )\n","      nFeatures.pop(i)\n","      nLabels.pop(i)\n","\n","  print(\"Length Original: \", len(lfeature), \" New Length: \", len(nFeatures) )\n","  return nFeatures, nLabels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ZYzIpFBidjb","colab_type":"code","colab":{}},"source":["def printTokensInfo(c, ct, cts, ctw):\n","  print('# To see the first element LENGTH')\n","  print(\"Corpus    : \" + str(len(c[0])) + \" \" + c[0][:100])\n","  print(\"Token     : \" + str(len(ct[0])) + \" \" + ct[0][:100])\n","  print(\"Token Stem: \" + str(len(cts[0])) + \" \" + cts[0][:100])\n","  print(\"Token W2V : \" + str(len(cts[0])) + \" \" + str(cts[0][:100]) )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0JvFoyG11FM","colab_type":"code","colab":{}},"source":["def binarize_Label_Organize( llabels ):\n","  llabels1 = label_binarize(llabels, classes=['MEDIUM-LOW', 'HIGH'])\n","  llabels2=[]\n","  for i in llabels1:\n","    llabels2.append(i[0])\n","  return llabels2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5GFufOnCrVnA","colab_type":"code","colab":{}},"source":["#Glove\n","def createGloveDict(txtFile=DATASETS_FOLDER + \"glove_s300.txt\", resultFile=DATASETS_FOLDER +'GloveDict.pkl'): \n","  #Load from TXT file\n","  glove_model = {}\n","  with open(DATASETS_FOLDER + \"glove_s300.txt\", 'r') as f:\n","      for line in f:\n","          # print(line)\n","          values = line.split()\n","          word = values[0]\n","          try:\n","            vector = np.asarray(values[1:], \"float32\")\n","            glove_model[word] = vector\n","          except:\n","            pass\n","\n","  #Save to Pickle File\n","  with open(DATASETS_FOLDER +'GloveDict.pkl', 'wb') as handle:\n","    pickle.dump(glove_model, handle)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MM2jFltOg3BK","colab_type":"text"},"source":["#OVERSampling"]},{"cell_type":"code","metadata":{"id":"Kp_852WWg2Tx","colab_type":"code","colab":{}},"source":["def overSamplingRISKs(lcorpus, llabels, percentage=0.3):\n","  #OVERSAMPLING HIGH RISKs\n","  # concatenate our training data back together\n","  X = pd.DataFrame(list(zip(lcorpus, llabels)), \n","                columns =['ConstText', 'RISK']) \n","\n","  # separate minority and majority classes\n","  not_HIGH = X[X.RISK!=\"HIGH\"]\n","  HIGH = X[X.RISK==\"HIGH\"]\n","\n","  # OVERsample minority\n","  high_upsampled = resample(HIGH,\n","                            replace=True, # sample with replacement\n","                            n_samples=int(round( len(not_HIGH)*percentage/(1-percentage) )), # match number in majority class\n","                            random_state=27) # reproducible results\n","\n","  # combine majority and upsampled minority\n","  upsampled = pd.concat([not_HIGH, high_upsampled])\n","\n","  #Recreate TRAIN_Corpus and TRAIN_LABELS\n","  lcorpus1=upsampled.ConstText.to_list()\n","  llabels1=upsampled.RISK.to_list()\n","\n","  # check new class counts\n","  print( upsampled.RISK.value_counts() )\n","\n","  return lcorpus1, llabels1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"16Bwe98jhDlA","colab_type":"text"},"source":["#Train Model"]},{"cell_type":"code","metadata":{"id":"AnnWcCxIg6MN","colab_type":"code","colab":{}},"source":["def get_metrics(true_labels, predicted_labels):\n","  acc = np.round(metrics.accuracy_score(true_labels,predicted_labels),2)\n","  pre = np.round(metrics.precision_score(true_labels,predicted_labels,average='binary'),2)\n","  rec = np.round(metrics.recall_score(true_labels,predicted_labels,average='binary'),2)\n","  f1s = np.round(metrics.f1_score(true_labels,predicted_labels,average='binary'),2)\n","\n","  print ('Accuracy:', acc, ' Precision:', pre, ' Recall:', rec, ' F1 Score:', f1s )\n","  return acc, pre, rec, f1s\n","\n","def train_predict_evaluate_model(classifier, train_features, train_labels1, test_features, test_labels1):\n","  classifier.fit(train_features, train_labels1) #model\n","  predictions = classifier.predict(train_features) \n","  acc, pre, rec, f1s = get_metrics(train_labels1,  predictions)\n","  predictions = classifier.predict(test_features) \n","  accTest, preTest, recTest, f1sTest = get_metrics(test_labels1,  predictions)\n","  return acc, pre, rec, f1s, accTest, preTest, recTest, f1sTest \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVgRVuRm-0PB","colab_type":"code","colab":{}},"source":["# LSTM\n","def train_LSTM(train, train_labels, test, test_labels):\n","  \n","  labels = np.asarray(train_labels )\n","  labels_test = np.asarray(test_labels) \n","\n","  #MODEL\n","  emdedding_size = 32\n","\n","  model = keras.Sequential([\n","    keras.layers.Bidirectional( keras.layers.LSTM(units=emdedding_size, input_shape=(n_features,1)) ),\n","    keras.layers.Dense(emdedding_size, activation='relu'),\n","    keras.layers.Dense(units=2, activation='softmax')\n","  ]) \n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","  # model.summary()\n","\n","  model.fit(train, labels, epochs=50, validation_data=(test, labels_test))\n","\n","  return model\n","\n","\n","def train_LSTM_w2v(w2v_model, train, train_labels, test, test_labels):\n","\n","  # train = train_features - np.min(train_features)\n","  # test  = test_features - np.min(train_features)\n","  labels = np.asarray(train_labels )\n","  labels_test = np.asarray(test_labels)\n","\n","  #MODEL\n","  n_cols = train.shape[1]\n","  pretrained_weights = w2v_model.wv.syn0\n","  vocab_size, emdedding_size = pretrained_weights.shape\n","\n","  model = keras.Sequential([\n","    keras.layers.Embedding( input_dim = vocab_size,\n","        output_dim = emdedding_size,\n","        weights=[pretrained_weights],\n","        input_length=emdedding_size,\n","        input_shape=(n_cols, ), \n","        trainable=False,\n","        ), \n","    keras.layers.Bidirectional( keras.layers.LSTM(units=emdedding_size, dropout=0.2, recurrent_dropout=0.2) ),\n","    # keras.layers.Bidirectional( keras.layers.LSTM(units=emdedding_size) ),\n","    keras.layers.Dense(emdedding_size, activation='relu'),\n","    # Dropout-\n","    keras.layers.Dense(units=2, activation='softmax')\n","  ]) \n","\n","  model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","  model.fit(train, labels, epochs=50, validation_data=(test, labels_test))\n","\n","  return model\n","\n","def predict_evaluate_LSTM(model, train_features, train_labels, test_features, test_labels):\n","  # Use Tensorflow to take advantage of GPU.\n","  #Transform im NP Arrays \n","  # train = train_features - np.min(train_features)\n","  # test  = test_features - np.min(train_features)\n","  # labels = np.asarray(train_labels )\n","  # test_labels = np.asarray(test_labels)\n","  \n","  predictionsPer = model.predict(train)\n","  predictions = np.asarray( [ np.argmax(x) for x in predictionsPer] )\n","  acc, pre, rec, f1s = get_metrics(labels,  predictions)\n","  \n","  predictionsPer = model.predict(test)\n","  predictions = np.asarray( [ np.argmax(x) for x in predictionsPer] )\n","  accTest, preTest, recTest, f1sTest = get_metrics(test_labels,  predictions)\n","\n","  return acc, pre, rec, f1s, accTest, preTest, recTest, f1sTest "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hV_BCPGamvgE","colab_type":"code","colab":{}},"source":["# Train/Execute the ML Models\n","def collect_Metrics_ML_Models(FEATURE_TYPE, FEATURE_TRAIN, LABELS_TRAIN, FEATURE_TEST, LABELS_TEST):\n","  l_metrics=[]\n","\n","  #Naive Bayes\n","  scaler = MinMaxScaler()\n","  scaler.fit(FEATURE_TRAIN)\n","  feature_matrix_TRAIN_NB = scaler.transform( FEATURE_TRAIN )\n","\n","  nbc = MultinomialNB(alpha=0.01)\n","  print('Naive Bayes:')\n","  acc, pre, rec, f1s, accTest, preTest, recTest, f1sTest = train_predict_evaluate_model(nbc, feature_matrix_TRAIN_NB, LABELS_TRAIN, FEATURE_TEST, LABELS_TEST )\n","  l_metrics.append([FEATURE_TYPE,'Naive Bayes',acc,pre,rec,f1s, accTest, preTest, recTest, f1sTest])\n","\n","  #Logistic Regression\n","  lgr = LogisticRegression(max_iter=1000)\n","  print('Logistic Regression:')\n","  acc, pre, rec, f1s, accTest, preTest, recTest, f1sTest = train_predict_evaluate_model(lgr, FEATURE_TRAIN, LABELS_TRAIN, FEATURE_TEST, LABELS_TEST )\n","  l_metrics.append([FEATURE_TYPE,'Logistic Regression',acc,pre,rec,f1s, accTest, preTest, recTest, f1sTest])\n","\n","  #SVM\n","  svm = SGDClassifier()\n","  print('SVM:')\n","  acc, pre, rec, f1s, accTest, preTest, recTest, f1sTest = train_predict_evaluate_model(svm, FEATURE_TRAIN, LABELS_TRAIN, FEATURE_TEST, LABELS_TEST )\n","  l_metrics.append([FEATURE_TYPE,'SVM',acc,pre,rec,f1s, accTest, preTest, recTest, f1sTest])\n","\n","  #RandomForest\n","  rfc = RandomForestClassifier(n_estimators=10)\n","  print('RandomForest:')\n","  acc, pre, rec, f1s, accTest, preTest, recTest, f1sTest = train_predict_evaluate_model(rfc, FEATURE_TRAIN, LABELS_TRAIN, FEATURE_TEST, LABELS_TEST )\n","  l_metrics.append([FEATURE_TYPE,'RandomForest',acc,pre,rec,f1s, accTest, preTest, recTest, f1sTest])\n","\n","  #NN\n","  nnc = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), max_iter=200, random_state=1)\n","  print('NN: ')\n","  acc, pre, rec, f1s, accTest, preTest, recTest, f1sTest = train_predict_evaluate_model(nnc, FEATURE_TRAIN, LABELS_TRAIN, FEATURE_TEST, LABELS_TEST )\n","  l_metrics.append([FEATURE_TYPE,'NN',acc,pre,rec,f1s, accTest, preTest, recTest, f1sTest])\n","\n","  # #LSTM\n","  # print('LSTM: ')\n","  # acc, pre, rec, f1s, accTest, preTest, recTest, f1sTest = predict_evaluate_LSTM(LSTM_MODEL, FEATURE_TRAIN, LABELS_TRAIN, FEATURE_TEST, LABELS_TEST )\n","  # l_metrics.append([FEATURE_TYPE,'NN',acc,pre,rec,f1s, accTest, preTest, recTest, f1sTest])\n","  \n","  return pd.DataFrame(l_metrics,columns=['Features','MLMethod','TRAIN-Accuracy','TRAIN-Precision','TRAIN-Recall','TRAIN-F1Score', 'TEST-Accuracy','TEST-Precision','TEST-Recall','TEST-F1Score'])"],"execution_count":0,"outputs":[]}]}